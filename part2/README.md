# Week 2

In this week, we will cover multi-armed bandits and markov decision processes. 
The topics we would like to cover this week are some topics related to MDP planning, and then dive into topics related to policy

## Lecture 10: Policy Iteration from [CS747](https://www.cse.iitb.ac.in/~shivaram/teaching/cs747-a2022/index.html)

There are two dynamic programming methods widely used to evaluate policies in MDPs. The first is value iteration, which we covered towards the end of last week and the other one if policy iteration. The final method for policy evaluation on MDPs is Linear Programming.

## Lecture 13: Prediction & Control, Basic Model-based Control from [CS747](https://www.cse.iitb.ac.in/~shivaram/teaching/cs747-a2022/index.html)

There are two major "tasks" in RL: prediction and control. Prediction involves approximating how well a particular policy performs on the MDP to gauge how good it is by calculating something like the action-value function(Q). In control, the goal is to find the optimal policy for the given MDP.

This lecture covers these two problems in the initial section and later introduces a simple and intuitive control algorithm called "model based control".

**NOTE:** Skip slides 6/16 to 9/16.

More content for `part2` will be uploaded soon.
