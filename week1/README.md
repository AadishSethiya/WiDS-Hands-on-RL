# Week 1

In this week, we will cover multi-armed bandits and markov decision processes. 

## Chapter 1 of Sutton and Barto (optional reading)

This chapter provides a very nice "birds eye view" of RL and will provide context to all the things that you will be reading about. That being said, this is an optional reading.

## Chapter 2 of Sutton and Barto

This chapter deals with the notion of multi-armed bandits. 

What are bandits? Imagine a slot machine with many "arms" -- typically, you pull one of these and get some reward. The problem here is figuring out which arm to pull, and the fact is that slot machines steal your money! Hence the term "bandits".


So, what is fascinating about these bandits? 


**Challenge**: Can you come up with an algorithm which gives maximum *expected* reward over ALL bandit instances? 


This challenge, and the connections between bandits and RL will become more and more apparent as you study this chapter.
